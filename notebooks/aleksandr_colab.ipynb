{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# команды для того, чтобы не баговался импорт функций и классов из других файлов\n",
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# основной импорт\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# импорт для работы с нейронными сетями (nn) и для классов оптимизаторов (Adam, SGD)\n",
    "from torch import nn, optim\n",
    "\n",
    "# импорт функций, которые не имеют обучаемых параметров relu, siftmax и др\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# импорт уже готовых датасетов и трансформаций (нормализация, обрезка и тд.)\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "# Dataset - базовый класс для работы с датасетами, можно создать свои, наследуясь от него\n",
    "# DataLoader - для создания лоадеров - разделенных на батчи данных\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# стандартная библиотека для работы со случайными числами\n",
    "import random\n",
    "\n",
    "# стандартный импорт нампая\n",
    "import numpy as np\n",
    "\n",
    "# библиотека OpenCV для обработки изображений\n",
    "import cv2\n",
    "\n",
    "# готовая функция для вычисления метрики IoU\n",
    "# from torchmetrics.functional.detection.iou import intersection_over_union\n",
    "from torchmetrics.functional import jaccard_index\n",
    "\n",
    "# импорты для визуализации и стилизации\n",
    "import matplotlib.pyplot as plt\n",
    "# import mplcyberpunk\n",
    "# plt.style.use(\"cyberpunk\")\n",
    "\n",
    "# torchutils для проверки модели на размерности\n",
    "import torchutils as tu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x105f8e410>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# фикс рандомности\n",
    "torch.manual_seed(52)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Зададим архитектуру модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The argument n_class specifies the number of classes for the segmentation task.\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        # -------\n",
    "        # Каждый блок в encoder состоит из двух сверточных слоев,\n",
    "        # за которыми следует max-pooling, за исключением последнего блока.\n",
    "        # ------- \n",
    "        # Используем padding=1 как best practice и для сохранения размеров карт \n",
    "        # признаков после сверток, облегчения реализации skip-connection, \n",
    "        # убираем необходимость пост-обработки выходного изображения (размеров)\n",
    "        # -------\n",
    "        # input: 640x640x3\n",
    "        self.e11 = nn.Conv2d(3, 64, kernel_size=3, padding=1) # output: 640x640x64\n",
    "        self.e12 = nn.Conv2d(64, 64, kernel_size=3, padding=1) # output: 640x640x64\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 320x320x64\n",
    "\n",
    "        # input: 320x320x64\n",
    "        self.e21 = nn.Conv2d(64, 128, kernel_size=3, padding=1) # output: 320x320x128\n",
    "        self.e22 = nn.Conv2d(128, 128, kernel_size=3, padding=1) # output: 320x320x128\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 160x160x128\n",
    "\n",
    "        # input: 160x160x128\n",
    "        self.e31 = nn.Conv2d(128, 256, kernel_size=3, padding=1) # output: 160x160x256\n",
    "        self.e32 = nn.Conv2d(256, 256, kernel_size=3, padding=1) # output: 160x160x256\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 80x80x256\n",
    "\n",
    "        # input: 80x80x256\n",
    "        self.e41 = nn.Conv2d(256, 512, kernel_size=3, padding=1) # output: 80x80x512\n",
    "        self.e42 = nn.Conv2d(512, 512, kernel_size=3, padding=1) # output: 80x80x512\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 40x40x512\n",
    "\n",
    "        # input: 40x40x512\n",
    "        self.e51 = nn.Conv2d(512, 1024, kernel_size=3, padding=1) # output: 40x40x1024\n",
    "        self.e52 = nn.Conv2d(1024, 1024, kernel_size=3, padding=1) # output: 40x40x1024\n",
    "\n",
    "\n",
    "        # Decoder\n",
    "        # -------\n",
    "        # Повышает размерность обратно до исходного изображения\n",
    "        # Каждый блок в декодировщике состоит из слоя апсемплинга,\n",
    "        # конкатенации с соответствующей картой признаков из encoder,\n",
    "        # и двух сверточных слоев\n",
    "        # -------\n",
    "        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2) # 40x40 -> 80x80\n",
    "        self.d11 = nn.Conv2d(1024, 512, kernel_size=3, padding=1) # Concatenated: 512 + 512 = 1024\n",
    "        self.d12 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2) # 80x80 -> 160x160\n",
    "        self.d21 = nn.Conv2d(512, 256, kernel_size=3, padding=1) # Concatenated: 256 + 256 = 512\n",
    "        self.d22 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2) # 160x160 -> 320x320\n",
    "        self.d31 = nn.Conv2d(256, 128, kernel_size=3, padding=1) # Concatenated: 128 + 128 = 256\n",
    "        self.d32 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2) # 320x320 -> 640x640\n",
    "        self.d41 = nn.Conv2d(128, 64, kernel_size=3, padding=1) # Concatenated: 64 + 64 = 128\n",
    "        self.d42 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        # Output layer\n",
    "        self.outconv = nn.Conv2d(64, n_class, kernel_size=1) # 640x640xn_class\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        xe11 = F.relu(self.e11(x)) # 640x640x64\n",
    "        xe12 = F.relu(self.e12(xe11)) # 640x640x64\n",
    "        xp1 = self.pool1(xe12) # 320x320x64\n",
    "\n",
    "        xe21 = F.relu(self.e21(xp1)) # 320x320x128\n",
    "        xe22 = F.relu(self.e22(xe21)) # 320x320x128\n",
    "        xp2 = self.pool2(xe22)  # 160x160x128\n",
    "\n",
    "        xe31 = F.relu(self.e31(xp2)) # 160x160x256\n",
    "        xe32 = F.relu(self.e32(xe31)) # 160x160x256\n",
    "        xp3 = self.pool3(xe32) # 80x80x256\n",
    "\n",
    "        xe41 = F.relu(self.e41(xp3)) # 80x80x512\n",
    "        xe42 = F.relu(self.e42(xe41)) # 80x80x512\n",
    "        xp4 = self.pool4(xe42) # 40x40x512\n",
    "\n",
    "        xe51 = F.relu(self.e51(xp4)) # 40x40x1024\n",
    "        xe52 = F.relu(self.e52(xe51)) # 40x40x1024\n",
    "        \n",
    "        # Decoder\n",
    "        xu1 = self.upconv1(xe52) # 80x80x512\n",
    "        xu11 = torch.cat([xu1, xe42], dim=1) # 80x80x1024\n",
    "        xd11 = F.relu(self.d11(xu11)) # 80x80x512\n",
    "        xd12 = F.relu(self.d12(xd11)) # 80x80x512\n",
    "\n",
    "        xu2 = self.upconv2(xd12) # 160x160x256\n",
    "        xu22 = torch.cat([xu2, xe32], dim=1) # 160x160x512\n",
    "        xd21 = F.relu(self.d21(xu22)) # 160x160x256\n",
    "        xd22 = F.relu(self.d22(xd21)) # 160x160x256\n",
    "\n",
    "        xu3 = self.upconv3(xd22) # 320x320x128\n",
    "        xu33 = torch.cat([xu3, xe22], dim=1) # 320x320x256\n",
    "        xd31 = F.relu(self.d31(xu33)) # 320x320x128\n",
    "        xd32 = F.relu(self.d32(xd31)) # 320x320x128\n",
    "\n",
    "        xu4 = self.upconv4(xd32) # 640x640x64\n",
    "        xu44 = torch.cat([xu4, xe12], dim=1) # 640x640x128\n",
    "        xd41 = F.relu(self.d41(xu44)) # 640x640x64\n",
    "        xd42 = F.relu(self.d42(xd41)) # 640x640x64\n",
    "\n",
    "        # Output layer\n",
    "        out = self.outconv(xd42) # 640x640xn_class\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Зададим фейковые данные для проверки корректности размерностей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# различные параметры для создания фейковых батчей данных\n",
    "# стандартное определение девайса\n",
    "# DEVICE = torch.device(\n",
    "#     'cuda' if torch.cuda.is_available() else\n",
    "#     'mps' if torch.backends.mps.is_available() else\n",
    "#     'cpu'\n",
    "# )\n",
    "DEVICE = 'cpu'\n",
    "\n",
    "# батчсайз определен 8 - больше не пролезало для теста на компуктере\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "# каналов = 3 потому что цветное изображение\n",
    "CHANNELS = 3\n",
    "\n",
    "# высота изображения 640 - надо ресайзить к этому\n",
    "HEIGHT = 640\n",
    "\n",
    "# ширина изображения 640 - надо ресайзить к этому\n",
    "WIDTH = 640\n",
    "\n",
    "# количество классов в датасете, т.к. лес рисуется белым, а не лес - черным \n",
    "# всего два класса, лес и не лес\n",
    "NUM_CLASSES = 1\n",
    "\n",
    "# создание фейковых наборов данных\n",
    "fake_pic = torch.randn(BATCH_SIZE, CHANNELS, HEIGHT, WIDTH, device=DEVICE)\n",
    "# fake_label = torch.randint(0, NUM_CLASSES, (BATCH_SIZE,), device=DEVICE)\n",
    "# fake_bbox = torch.rand(BATCH_SIZE, 4, device=DEVICE)\n",
    "\n",
    "# # создание общего фейкового батча\n",
    "# fake_batch = (fake_pic, fake_label, fake_bbox)\n",
    "\n",
    "# # примечание \n",
    "# # попробуем пока что передавать просто fake_pic, т.к. у нас модель принимает\n",
    "# # в функции forward только картинку (один тип элементов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Проверим модель на корректность размерностей на всех этапах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (e11): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (e12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (e21): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (e22): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (e31): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (e32): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (e41): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (e42): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (e51): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (e52): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (upconv1): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (d11): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (d12): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (upconv2): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (d21): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (d22): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (upconv3): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (d31): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (d32): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (upconv4): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (d41): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (d42): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (outconv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# создадим экземпляр класса модели\n",
    "model = UNet(n_class=NUM_CLASSES)\n",
    "# и отправим ее сразу на девайс\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================\n",
      "Layer              Kernel               Output          Params               FLOPs\n",
      "==================================================================================\n",
      "0_e11             [3, 64, 3, 3]    [4, 64, 640, 640]       1,792     2,936,012,800\n",
      "1_e12            [64, 64, 3, 3]    [4, 64, 640, 640]      36,928    60,502,835,200\n",
      "2_pool1                       -    [4, 64, 320, 320]           0                 0\n",
      "3_e21           [64, 128, 3, 3]   [4, 128, 320, 320]      73,856    30,251,417,600\n",
      "4_e22          [128, 128, 3, 3]   [4, 128, 320, 320]     147,584    60,450,406,400\n",
      "5_pool2                       -   [4, 128, 160, 160]           0                 0\n",
      "6_e31          [128, 256, 3, 3]   [4, 256, 160, 160]     295,168    30,225,203,200\n",
      "7_e32          [256, 256, 3, 3]   [4, 256, 160, 160]     590,080    60,424,192,000\n",
      "8_pool3                       -     [4, 256, 80, 80]           0                 0\n",
      "9_e41          [256, 512, 3, 3]     [4, 512, 80, 80]   1,180,160    30,212,096,000\n",
      "10_e42         [512, 512, 3, 3]     [4, 512, 80, 80]   2,359,808    60,411,084,800\n",
      "11_pool4                      -     [4, 512, 40, 40]           0                 0\n",
      "12_e51        [512, 1024, 3, 3]    [4, 1024, 40, 40]   4,719,616    30,205,542,400\n",
      "13_e52       [1024, 1024, 3, 3]    [4, 1024, 40, 40]   9,438,208    60,404,531,200\n",
      "14_upconv1    [512, 1024, 2, 2]     [4, 512, 80, 80]   2,097,664    53,700,198,400\n",
      "15_d11        [1024, 512, 3, 3]     [4, 512, 80, 80]   4,719,104   120,809,062,400\n",
      "16_d12         [512, 512, 3, 3]     [4, 512, 80, 80]   2,359,808    60,411,084,800\n",
      "17_upconv2     [256, 512, 2, 2]   [4, 256, 160, 160]     524,544    53,713,305,600\n",
      "18_d21         [512, 256, 3, 3]   [4, 256, 160, 160]   1,179,904   120,822,169,600\n",
      "19_d22         [256, 256, 3, 3]   [4, 256, 160, 160]     590,080    60,424,192,000\n",
      "20_upconv3     [128, 256, 2, 2]   [4, 128, 320, 320]     131,200    53,739,520,000\n",
      "21_d31         [256, 128, 3, 3]   [4, 128, 320, 320]     295,040   120,848,384,000\n",
      "22_d32         [128, 128, 3, 3]   [4, 128, 320, 320]     147,584    60,450,406,400\n",
      "23_upconv4      [64, 128, 2, 2]    [4, 64, 640, 640]      32,832    53,791,948,800\n",
      "24_d41          [128, 64, 3, 3]    [4, 64, 640, 640]      73,792   120,900,812,800\n",
      "25_d42           [64, 64, 3, 3]    [4, 64, 640, 640]      36,928    60,502,835,200\n",
      "26_outconv        [64, 1, 1, 1]     [4, 1, 640, 640]          65       106,496,000\n",
      "==================================================================================\n",
      "Total params: 31,031,745\n",
      "Trainable params: 31,031,745\n",
      "Non-trainable params: 0\n",
      "Total FLOPs: 1,366,243,737,600 / 1366.24 GFLOPs\n",
      "----------------------------------------------------------------------------------\n",
      "Input size (MB): 18.75\n",
      "Forward/backward pass size (MB): 7987.50\n",
      "Params size (MB): 118.38\n",
      "Estimated Total Size (MB): 8124.63\n",
      "==================================================================================\n"
     ]
    }
   ],
   "source": [
    "# отправим в модель фейковую пачку данных и посмотрим результат\n",
    "tu.get_model_summary(model, fake_pic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Зададим оптимизатор и определим loss-функцию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# так как бинарная классификация, используем бинарную кросс-энтропию\n",
    "# используем на логитах, потому что в архитектуре на выходном слое сети \n",
    "# отсутствует функция активации sigmoid, а обычный BCELoss() ожидает на вход\n",
    "# диапазон чисел от 0 до 1. \n",
    "# Если брать BCEWithLogitsLoss() то там уже под капотом применяется сигмоида\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# оптимизатор - используем мессию\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Получим и подготовим данные для обучения модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Директория ../data уже существует. Пропуск скачивания.\n"
     ]
    }
   ],
   "source": [
    "# подгрузим с помощью апи кагла\n",
    "\n",
    "dataset_dir = '../data'\n",
    "zip_file = '../data/augmented-forest-segmentation.zip'\n",
    "train_img_dir = '../data/unet-for-train/images/train'\n",
    "valid_img_dir = '../data/unet-for-train/images/valid'\n",
    "train_mask_dir = '../data/unet-for-train/masks/train'\n",
    "valid_mask_dir = '../data/unet-for-train/masks/valid'\n",
    "\n",
    "if not os.path.exists(dataset_dir):\n",
    "    # загрузим датасет с Kaggle\n",
    "    !kaggle datasets download -d quadeer15sh/augmented-forest-segmentation -p {dataset_dir}/\n",
    "\n",
    "    # распакуем zip-файл с помощью команды unzip\n",
    "    !unzip -qq {zip_file} -d {dataset_dir}/\n",
    "\n",
    "    # удалим zip-файл после распаковки\n",
    "    !rm {zip_file}\n",
    "\n",
    "    print(f'Датасет успешно скачан и распакован в папку {dataset_dir}')\n",
    "else:\n",
    "    print(f'Директория {dataset_dir} уже существует. Пропуск скачивания.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# добавим необходимые импорты для работы с данными\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10452_sat_08.jpg</td>\n",
       "      <td>10452_mask_08.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10452_sat_18.jpg</td>\n",
       "      <td>10452_mask_18.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>111335_sat_00.jpg</td>\n",
       "      <td>111335_mask_00.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>111335_sat_01.jpg</td>\n",
       "      <td>111335_mask_01.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>111335_sat_02.jpg</td>\n",
       "      <td>111335_mask_02.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               image                mask\n",
       "0   10452_sat_08.jpg   10452_mask_08.jpg\n",
       "1   10452_sat_18.jpg   10452_mask_18.jpg\n",
       "2  111335_sat_00.jpg  111335_mask_00.jpg\n",
       "3  111335_sat_01.jpg  111335_mask_01.jpg\n",
       "4  111335_sat_02.jpg  111335_mask_02.jpg"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Определяем директорию с данными\n",
    "data_dir = '../data'\n",
    "\n",
    "# Путь к CSV-файлу\n",
    "csv_file = os.path.join(data_dir, 'meta_data.csv')\n",
    "\n",
    "# Читаем CSV-файл в DataFrame\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Выводим первые несколько строк DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучающая выборка: 4086 изображений с масками\n",
      "Валидационная выборка: 1022 изображений с масками\n"
     ]
    }
   ],
   "source": [
    "# Разделяем DataFrame на обучающую и валидационную выборки (80% и 20% соответственно)\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Сбрасываем индексы в DataFrame\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "\n",
    "# Выводим количество примеров в каждой выборке\n",
    "print(f'Обучающая выборка: {len(train_df)} изображений с масками')\n",
    "print(f'Валидационная выборка: {len(val_df)} изображений с масками')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Определение класса Dataset для загрузки изображений и масок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, df, images_dir, masks_dir):\n",
    "        self.df = df\n",
    "        self.images_dir = images_dir\n",
    "        self.masks_dir = masks_dir\n",
    "\n",
    "        self.image_transform = transforms.Compose([\n",
    "            transforms.Resize((640, 640)),\n",
    "            transforms.ToTensor()  # Масштабирует значения в [0.0, 1.0]\n",
    "        ])\n",
    "\n",
    "        self.mask_transform = transforms.Compose([\n",
    "            transforms.Resize((640, 640)),\n",
    "            transforms.ToTensor()  # Масштабирует значения в [0.0, 1.0]\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.df.loc[idx, 'image']\n",
    "        mask_name = self.df.loc[idx, 'mask']\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        mask_path = os.path.join(self.masks_dir, mask_name)\n",
    "\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        mask = Image.open(mask_path).convert('L')\n",
    "\n",
    "        image = self.image_transform(image)\n",
    "        mask = self.mask_transform(mask)\n",
    "\n",
    "        # Преобразуем маску в бинарный формат\n",
    "        mask = (mask > 0).float()\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Создание объектов DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir = os.path.join(data_dir, 'Forest_Segmented/Forest_Segmented/images')\n",
    "masks_dir = os.path.join(data_dir, 'Forest_Segmented/Forest_Segmented/masks')\n",
    "\n",
    "train_dataset = SegmentationDataset(train_df, images_dir, masks_dir)\n",
    "val_dataset = SegmentationDataset(val_df, images_dir, masks_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Создание объектов DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Напишем цикл обучения модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(\n",
    "    model: nn.Module,\n",
    "    n_epochs: int,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    train_loader: DataLoader,\n",
    "    valid_loader: DataLoader,\n",
    "    log=None\n",
    ") -> dict:\n",
    "    \n",
    "    # если не приняли с вызовом функции нужный словарь, то инициализируем его\n",
    "    if log is None:\n",
    "        log = dict()\n",
    "        log['epoch_train_loss'] = []\n",
    "        log['epoch_valid_loss'] = []\n",
    "        log['epoch_train_iou'] = []\n",
    "        log['epoch_valid_iou'] = []\n",
    "    \n",
    "    # определяем текущую эпоху обучения\n",
    "    start_epoch = len(log['epoch_train_loss'])\n",
    "    # и запускаем цикл обучения\n",
    "    for epoch in range(start_epoch+1, start_epoch+n_epochs+1):\n",
    "        print(f'{\"-\"*47} Epoch {epoch} {\"-\"*47}')\n",
    "        \n",
    "        batch_loss = []\n",
    "        batch_iou = []\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        for images, masks in train_loader:\n",
    "            images = images.to(DEVICE)\n",
    "            masks = masks.to(DEVICE)\n",
    "            \n",
    "            logits = model(images)\n",
    "            \n",
    "            loss = criterion(logits, masks)\n",
    "            \n",
    "            # Применяем sigmoid и бинаризуем прогнозы\n",
    "            preds = torch.sigmoid(logits)\n",
    "            preds = (preds > 0.5).float()\n",
    "            iou = jaccard_index(preds, masks,task='binary')\n",
    "            \n",
    "            batch_loss.append(loss.item())\n",
    "            batch_iou.append(iou.item())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        log['epoch_train_loss'].append(np.mean(batch_loss))\n",
    "        log['epoch_train_iou'].append(np.mean(batch_iou))\n",
    "        \n",
    "        batch_loss = []\n",
    "        batch_iou = []\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        for images, masks in valid_loader:\n",
    "            images = images.to(DEVICE)\n",
    "            masks = masks.to(DEVICE)\n",
    "            \n",
    "            with torch.inference_mode():\n",
    "                logits = model(images)\n",
    "            \n",
    "            loss = criterion(logits, masks)\n",
    "            \n",
    "            # Применяем sigmoid и бинаризуем прогнозы\n",
    "            preds = torch.sigmoid(logits)\n",
    "            preds = (preds > 0.5).float()\n",
    "            iou = jaccard_index(preds, masks,task='binary')\n",
    "           \n",
    "            batch_loss.append(loss.item())\n",
    "            batch_iou.append(iou.item())\n",
    "        \n",
    "        log['epoch_valid_loss'].append(np.mean(batch_loss))\n",
    "        log['epoch_valid_iou'].append(np.mean(batch_iou))\n",
    "        \n",
    "        print(f\"Train stage: loss: {log['epoch_train_loss'][-1]:.3f}, iou: {log['epoch_train_iou'][-1]:.3f}\")\n",
    "        print(f\"Valid stage: loss: {log['epoch_valid_loss'][-1]:.3f}, iou: {log['epoch_valid_iou'][-1]:.3f}\")\n",
    "        \n",
    "        # Отображение результатов после каждой эпохи\n",
    "        # Получаем один батч из валидационного загрузчика\n",
    "        images, masks = next(iter(valid_loader))\n",
    "        images = images.to(DEVICE)\n",
    "        masks = masks.to(DEVICE)\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            logits = model(images)\n",
    "            preds = torch.sigmoid(logits)\n",
    "            preds = (preds > 0.5).float()\n",
    "        \n",
    "        # Случайный индекс картинки в батче\n",
    "        random_img_index = np.random.randint(0, images.size(0))\n",
    "        \n",
    "        # Подготовка данных для отображения\n",
    "        img = images[random_img_index].cpu().permute(1, 2, 0).numpy()\n",
    "        img = (img * 255).astype(np.uint8)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        pred_mask = preds[random_img_index][0].cpu().numpy()\n",
    "        true_mask = masks[random_img_index][0].cpu().numpy()\n",
    "        \n",
    "        # Отображение изображений\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        axs[0].imshow(img)\n",
    "        axs[0].set_title('Original Image')\n",
    "        axs[0].axis('off')\n",
    "        \n",
    "        axs[1].imshow(pred_mask, cmap='gray')\n",
    "        axs[1].set_title('Predicted Mask')\n",
    "        axs[1].axis('off')\n",
    "        \n",
    "        axs[2].imshow(true_mask, cmap='gray')\n",
    "        axs[2].set_title('True Mask')\n",
    "        axs[2].axis('off')\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------- Epoch 1 -----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "first_log = fit(\n",
    "    model=model,\n",
    "    n_epochs=1,\n",
    "    optimizer=optimizer,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
